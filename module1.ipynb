{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NavMohan-24/Machine-Learning-for-Semiconductor-Quantum-Devices/blob/main/module1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6845330",
      "metadata": {
        "id": "f6845330"
      },
      "source": [
        "# Module 1: Supervised learning for quantum dot configuration tuning\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63920a67",
      "metadata": {
        "id": "63920a67"
      },
      "source": [
        "### Table of Contents\n",
        "\n",
        "* [1. Charge stability diagram classification with dense Neural Networks](#part1)\n",
        "    * [1.1. Import data and analyze the data shape](#Step_1)\n",
        "    * [1.2. Prepare data](#Step_2)\n",
        "    * [1.3. Setup the model](#Step_3)\n",
        "    * [1.4. Compile the model](#Step_4)\n",
        "    * [1.5. Train the model](#Step_5)\n",
        "    * [1.6. Interpretation and analysis](#Step_6)\n",
        "\n",
        "\n",
        "* [2. Exercise](#part2)\n",
        "\n",
        "\n",
        "* [3. Charge stability diagram classification with Convolutional Neural Networks](#part3)\n",
        "    * [3.1. Import data and analyze the data shape](#Step_1)\n",
        "    * [3.2. Prepare data](#Step_2)\n",
        "    * [3.3. Setup the model](#Step_3)\n",
        "    * [3.4. Compile and train the model](#Step_4)\n",
        "    * [3.5. Interpretation and analysis](#Step_5)\n",
        "\n",
        "\n",
        "* [4. Exercise](#part4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82138bab",
      "metadata": {
        "id": "82138bab"
      },
      "source": [
        "## 1. Charge stability diagram classification with dense Neural Networks <a class=\"anchor\" id=\"part1\"></a>\n",
        "In this notebook we learn how to classify the charge stability diagram of single and double quantum dots with the help of Neural Networks (NNs).\n",
        "\n",
        "The NN models that we are considering here are the fully connected neural networks and the Convolutional Neural Networks.\n",
        "  \n",
        "**Dataset**: The configurations are generated by our simulator on a 50x50 square lattice for both noisy and noiseless cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff790b69",
      "metadata": {
        "id": "ff790b69"
      },
      "outputs": [],
      "source": [
        "!git clone https://gitlab.com/QMAI/mlqe_2023_edx.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f3dd415",
      "metadata": {
        "id": "6f3dd415"
      },
      "source": [
        "### 1.1. Import data and analyze the data shape <a class=\"anchor\" id=\"Step_1\"></a>\n",
        "\n",
        "The folder `dataset` contains the data for noisy and noiseless charge stability diagrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5f0be9",
      "metadata": {
        "id": "7d5f0be9"
      },
      "outputs": [],
      "source": [
        "# Helper Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import scipy as sp\n",
        "from scipy.special import softmax\n",
        "# Machine learning related libraries:\n",
        "import torch\n",
        "import torch.nn as nn            # base class used to develop all neural network models\n",
        "import torch.nn.functional as F  # module of relu activation functions\n",
        "import torch.optim as optim      # module of Adam optimizer\n",
        "import glob\n",
        "# from itertools import chain      # append two range() functions\n",
        "from torch.utils.data import DataLoader # easy and organized data loading to the ML model\n",
        "from torch.utils.data import Dataset #for nice loadable dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17723562",
      "metadata": {
        "id": "17723562"
      },
      "outputs": [],
      "source": [
        "####### Detect if running on the clusters  #######\n",
        "# use CUDA:\n",
        "torch.cuda.is_available()\n",
        "print(\"Is cuda available?\", torch.cuda.is_available())\n",
        "\n",
        "# set a flag\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8c5d94",
      "metadata": {
        "id": "bd8c5d94"
      },
      "source": [
        "In supervised learning we need to have labeled data, in the cell below we load the data and the respective labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7eaa39",
      "metadata": {
        "id": "7e7eaa39"
      },
      "outputs": [],
      "source": [
        "with open('mlqe_2023_edx/week3/dataset/csds.npy', 'rb') as f:\n",
        "    data_noisy = np.load(f)\n",
        "\n",
        "with open('mlqe_2023_edx/week3/dataset/csds_noiseless.npy', 'rb') as f:\n",
        "    data = np.load(f)\n",
        "\n",
        "with open('mlqe_2023_edx/week3/dataset/labels.npy', 'rb') as f:\n",
        "    labels = np.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f735cc",
      "metadata": {
        "id": "00f735cc"
      },
      "source": [
        "We have two sets of data, one is a noiseless (ideal) dataset and the other is a noisy dataset, similar to what we expect from an experiment. First we use the ideal dataset for training our system. You can also use the other set or change the size of the dataset, if you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50abaddf",
      "metadata": {
        "id": "50abaddf"
      },
      "outputs": [],
      "source": [
        "number_of_data = 200\n",
        "data = data[:number_of_data]\n",
        "labels = labels[:number_of_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8085f177",
      "metadata": {
        "id": "8085f177"
      },
      "source": [
        "Let'a visualize our data to see how each of phase diagram look like with their corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ea9e94",
      "metadata": {
        "id": "09ea9e94"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig, ax = plt.subplots(1, 10, figsize = (20,10))\n",
        "for index, d in enumerate(data[np.random.choice(len(data), size = 10)]):\n",
        "    ax[index].imshow(data[index])\n",
        "    ax[index].axis('off')\n",
        "    ax[index].set_title(f'Label: {(labels[index])}')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ade2adc",
      "metadata": {
        "id": "7ade2adc"
      },
      "source": [
        "### 1.2. Prepare data <a class=\"anchor\" id=\"Step_2\"></a>\n",
        "Now that we know how our data set looks like, we need to make it readable for PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cc8eace",
      "metadata": {
        "id": "0cc8eace"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.Tensor(data)\n",
        "        self.labels = torch.Tensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_idx = self.data[idx]\n",
        "        label = self.labels[idx].type(torch.LongTensor)\n",
        "        return data_idx, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767c87fb",
      "metadata": {
        "id": "767c87fb"
      },
      "source": [
        "Let's keep 20 percent of them for test set and 80 percent for train set and fix the batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a687089c",
      "metadata": {
        "id": "a687089c"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(data, labels)\n",
        "trainset, testset = torch.utils.data.random_split(dataset, (int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)))\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size = batch_size)\n",
        "testloader = DataLoader(testset, batch_size = batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77473348",
      "metadata": {
        "id": "77473348"
      },
      "source": [
        "Then we can check the shape of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac5a380",
      "metadata": {
        "id": "cac5a380"
      },
      "outputs": [],
      "source": [
        "for X, y in trainloader:\n",
        "    print(f\"Shape of X: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    print(y)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d15d8ab3",
      "metadata": {
        "id": "d15d8ab3"
      },
      "source": [
        "### 1.3. Setup the model <a class=\"anchor\" id=\"Step_3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1badf342",
      "metadata": {
        "id": "1badf342"
      },
      "source": [
        "Finally, we can set up the model!\n",
        "\n",
        "One example of how to do that: input size is 50 x 50 = 2500, followed by one fully connected layer with _ReLU_ activation function and binary (1/0) output.\n",
        "\n",
        "We define a class for the neural network. _torch_ requires this class to have a _.forward(x)_ method, and it is a convention to define the network during initialization of the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df076ec",
      "metadata": {
        "id": "4df076ec"
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork,self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(50*50,16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16,2),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# here we initialize an instance of the class\n",
        "model = NeuralNetwork().to(device)\n",
        "# print the status of the model\n",
        "# the print command is inherited from nn.Module in the definition of the network\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a82eca",
      "metadata": {
        "id": "f6a82eca"
      },
      "source": [
        "We can see the number of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73436abc",
      "metadata": {
        "id": "73436abc"
      },
      "outputs": [],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {pytorch_total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4f28ca",
      "metadata": {
        "id": "5b4f28ca"
      },
      "source": [
        "### 1.4. Compile the model <a class=\"anchor\" id=\"Step_4\"></a>\n",
        "Now that the model is defined, we need to train our model. But before doing so, there are a few details we need to specify.\n",
        "\n",
        "\n",
        "1.   Loss function: we need to choose what function we want our model to minimise e.g. mean square, cross entropy.\n",
        "2.   Optimisation method: How we want to update the weights e.g. stochastic gradient descent, ADAM.\n",
        "3.   Metrics: some quantity we want to keep track of while we are training, e.g. value of the loss function or the accuracy of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a8e24e2",
      "metadata": {
        "id": "6a8e24e2"
      },
      "outputs": [],
      "source": [
        "# optimizing the model parameters\n",
        "# to train the model we need a LOSS FUNCTION and an OPTIMIZER\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # we use Cross Entropy as loss\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) # The optimizer is Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3dc8d4d",
      "metadata": {
        "id": "f3dc8d4d"
      },
      "outputs": [],
      "source": [
        "# define the train and test loss accumulation\n",
        "train_loss_dnn = []\n",
        "train_acc_dnn = []\n",
        "test_loss_dnn = []\n",
        "test_acc_dnn = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4e5f1c4",
      "metadata": {
        "id": "c4e5f1c4"
      },
      "outputs": [],
      "source": [
        "# define training function: make predicition on data set batch,\n",
        "# backpropagate the error and adjust model parameters\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, train_loss, train_acc):\n",
        "    num_batches = len(dataloader)\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    running_loss, correct = 0, 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        #compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred,y)\n",
        "\n",
        "        #Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # collect the accuracy:\n",
        "        running_loss += loss.item()\n",
        "        correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "\n",
        "        if batch % 25==0:\n",
        "            loss,current = loss.item(), batch*len(X)\n",
        "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    running_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    train_acc.append(correct)\n",
        "    train_loss.append(running_loss)\n",
        "\n",
        "#check performance against the test data set\n",
        "def test(dataloader,model,loss_fn,test_loss, test_acc):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    running_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X,y in dataloader:\n",
        "            X,y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            running_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "    running_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    test_acc.append(correct)\n",
        "    test_loss.append(running_loss)\n",
        "    print(f\"Test Error: \\n Accuracy {(100*correct):>0.1f}%, Avg loss:{running_loss:>8f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b92676b4",
      "metadata": {
        "id": "b92676b4"
      },
      "source": [
        "### 1.5. Train the model <a class=\"anchor\" id=\"Step_5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34cac417",
      "metadata": {
        "id": "34cac417"
      },
      "source": [
        "### Important note\n",
        "\n",
        "In case you want to retrain your model for any purposes, you must **restart** you kernell first because you previous training data is already saved on your memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3732e003",
      "metadata": {
        "id": "3732e003"
      },
      "source": [
        "We can then use the defined functions to train the model for a certain number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a002712",
      "metadata": {
        "id": "1a002712"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n -------------------\")\n",
        "    train(trainloader,model,loss_fn,optimizer, train_loss_dnn, train_acc_dnn)\n",
        "    test(testloader,model,loss_fn, test_loss_dnn, test_acc_dnn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22832dd6",
      "metadata": {
        "id": "22832dd6"
      },
      "source": [
        "### 1.6. Interpretation and analysis <a class=\"anchor\" id=\"Step_6\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00e21998",
      "metadata": {
        "id": "00e21998"
      },
      "source": [
        "Here we define a function to plot the training and test accuracy/loss, which let us analyse the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a776672",
      "metadata": {
        "id": "6a776672"
      },
      "outputs": [],
      "source": [
        "def create_acc_loss_graph(train_acc,train_loss, test_acc, test_loss):\n",
        "    fig, axes = plt.subplots(ncols=2, nrows=1, dpi=300)\n",
        "    fig.set_size_inches(9, 3)\n",
        "    ax1, ax2 = axes[0], axes[1]\n",
        "\n",
        "    ax1.plot(train_acc,'-o',label=\"train\", markersize=4)\n",
        "    ax1.plot(test_acc,'--+',label=\"test\", markersize=4)\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(loc=3)\n",
        "\n",
        "    ax2.plot(train_loss,'-o',label=\"train\", markersize=4)\n",
        "    ax2.plot(test_loss,'--+',label=\" test\", markersize=4)\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(loc=1)\n",
        "\n",
        "    #plot parameters\n",
        "    ax1.set_ylim(0, np.max([np.max(train_acc),np.max(test_acc)])+0.1)\n",
        "    ax2.set_ylim(-0.1, np.max([np.max(train_loss),np.max(test_loss)])+0.1)\n",
        "    ax1.grid(True, which='both',linewidth=0.1)\n",
        "    ax2.grid(True, which='both',linewidth=0.1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d54d8c5",
      "metadata": {
        "id": "0d54d8c5"
      },
      "outputs": [],
      "source": [
        "create_acc_loss_graph(train_acc_dnn,train_loss_dnn, test_acc_dnn, test_loss_dnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609b9226",
      "metadata": {
        "id": "609b9226"
      },
      "source": [
        "## 2. Exercise <a class=\"anchor\" id=\"part2\"></a>\n",
        "\n",
        "Now it is time to get your hands dirty and do some coding. Repeat the process above to train the model for noisy data. How the training changes?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b32164c",
      "metadata": {
        "id": "8b32164c"
      },
      "source": [
        "### Important note\n",
        "\n",
        "In case you want to retrain your model for any purposes, you must **restart** you kernell first because you previous training data is already saved on your memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6af7f9ed",
      "metadata": {
        "id": "6af7f9ed"
      },
      "source": [
        "## 3. Charge stability diagram Classification with Convolutional Neural Networks (CNN) <a class=\"anchor\" id=\"part3\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67dfd819",
      "metadata": {
        "id": "67dfd819"
      },
      "source": [
        "We have introduced here a new layer. Lets briefly understand what each layer does.\n",
        "\n",
        "1.  **Convolutional**: This layer applies 32 kernels of size 2 by 2 over the input image. There are 2 paddings one can choose from 'valid' or 'same'. For our purpose, we need periodic boundary conditions and we thus use 'Valid', which means it does not add additional 'pixels' around the configuration.\n",
        "```\n",
        "torch.nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride=1, padding=1)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For more information about layer check out the PyTorch documentation: https://pytorch.org/docs/stable/nn\n",
        "\n",
        "\n",
        "One final note on the input shape: While for the (first) dense layer we can define just about any shape, the input shape for the convolutional layer is necessarily N x M x C, where C is the number of channels. If the input available has only a single channel, i.e., its shape is N x M, an additional axis with dimension 1 needs to be added for it to work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bea2352",
      "metadata": {
        "id": "7bea2352"
      },
      "source": [
        "Animation: What convolutional layers do?\n",
        "\n",
        "![Alt Text](https://miro.medium.com/max/789/0*jLoqqFsO-52KHTn9.gif)\n",
        "\n",
        "The yellow matrix is called a kernel, and its size is one of the hyperparameters. It moves around the green (input) image with step defined by `stride` (here = 1), and how it behaves at the edges of the image is called `padding`. The resulting convolved image is an input to a next layer.\n",
        "\n",
        "Note that the convolution is performed simultaneously for each channel of the input image, e.g. a color image has C=3 channels, RGB: Red, Green, and Blue. The filters are set to have odd size for practical purpose CxFxF, e.g, 3x3x3, 3x5x5. The output of this operation is one scalar value, an artificial neuron. An illustrative animation for the convolution layer is given in the course by Fei-Fei Li. More details: http://cs231n.github.io/convolutional-networks/#conv.\n",
        "\n",
        "![Alt Text](https://miro.medium.com/max/1100/1*qtinjiZct2w7Dr4XoFixnA.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63e6d969",
      "metadata": {
        "id": "63e6d969"
      },
      "source": [
        "### 3.2. Prepare data <a class=\"anchor\" id=\"Step_2\"></a>\n",
        "Now let's prepare our data again, this time we choose only 100 data for training and 100 for test which is almost half of what we used in the dense neural network. Keep in mind that we changed the shape of the input data from [100,50,50] to [100,1,50,50] because CNN in the PyTorch mainly designed for image processing and images usually have 3 channels, RGB but here we only need 1 channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "789d4748",
      "metadata": {
        "id": "789d4748"
      },
      "outputs": [],
      "source": [
        "number_of_data = 100\n",
        "\n",
        "with open('mlqe_2023_edx/week3/dataset/csds.npy', 'rb') as f:\n",
        "    data_noisy = np.load(f)\n",
        "\n",
        "with open('mlqe_2023_edx/week3/dataset/csds_noiseless.npy', 'rb') as f:\n",
        "    data = np.load(f)\n",
        "\n",
        "with open('mlqe_2023_edx/week3/dataset/labels.npy', 'rb') as f:\n",
        "    labels = np.load(f)\n",
        "\n",
        "data_val = data[-number_of_data:]\n",
        "data_train = data[:number_of_data]\n",
        "\n",
        "labels_val = labels[-number_of_data:]\n",
        "labels_train = labels[:number_of_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff99f412",
      "metadata": {
        "id": "ff99f412"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(data_train, labels_train)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataset_val = CustomDataset(data_val, labels_val)\n",
        "data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdcc30d7",
      "metadata": {
        "id": "fdcc30d7"
      },
      "outputs": [],
      "source": [
        "for i, (x,y) in enumerate(data_loader):\n",
        "    if i == 1: break\n",
        "    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7def3de3",
      "metadata": {
        "id": "7def3de3"
      },
      "source": [
        "### 3.3. Setup the model <a class=\"anchor\" id=\"Step_3\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28022da3",
      "metadata": {
        "id": "28022da3"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding =1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(in_features=64 * 11 * 11, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 50, 50)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79dfb937",
      "metadata": {
        "id": "79dfb937"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 10\n",
        "\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30cc87a0",
      "metadata": {
        "id": "30cc87a0"
      },
      "source": [
        "### 3.4. Compile and train the model <a class=\"anchor\" id=\"Step_4\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "243e21eb",
      "metadata": {
        "id": "243e21eb"
      },
      "source": [
        "Clearing the memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef8fee2",
      "metadata": {
        "id": "7ef8fee2"
      },
      "outputs": [],
      "source": [
        "# Initialize the network\n",
        "model = Net().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "total_step = len(data_loader)\n",
        "train_ac = []\n",
        "train_Loss = []\n",
        "test_Loss = []\n",
        "test_ac = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(data_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #if (i + 1) % 10 == 0:\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item()}\")\n",
        "    train_Loss.append(loss.item())\n",
        "    # Calculate accuracy after each epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_data = torch.tensor(data_train).to(torch.float32).to(device)\n",
        "        target_data = torch.tensor(labels_train).to(torch.int)\n",
        "        print(input_data.shape)\n",
        "        outputs = model(input_data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (predicted == target_data).sum().item() / len(target_data)\n",
        "    train_ac.append(accuracy)\n",
        "    print(f\"Accuracy after Epoch {epoch+1}: {accuracy}\")\n",
        "\n",
        "    # Evaluation on test dataset\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_test_loss = 0.0\n",
        "    correct_test_predictions = 0\n",
        "\n",
        "    for images, labels in data_loader_val:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "            test_loss = criterion(outputs, labels)\n",
        "            running_test_loss += test_loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_test_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate accuracy and average loss for the test dataset\n",
        "    test_accuracy = correct_test_predictions / len(data_val)\n",
        "    test_ac.append(test_accuracy)\n",
        "    test_average_loss = running_test_loss / len(data_loader_val)\n",
        "    test_Loss.append(test_average_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_average_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4587af8b",
      "metadata": {
        "id": "4587af8b"
      },
      "source": [
        "### Important note\n",
        "\n",
        "In case you want to retrain your model for any purposes, you must **restart** you kernell first because you previous training data is already saved on your memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d8b3b2b",
      "metadata": {
        "id": "4d8b3b2b"
      },
      "source": [
        "### 1.5. Interpretation and analysis <a class=\"anchor\" id=\"Step_5\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53e3dde",
      "metadata": {
        "id": "f53e3dde"
      },
      "outputs": [],
      "source": [
        "def create_acc_loss_graph2(train_loss, train_acc, test_loss,test_acc):\n",
        "    fig, axes = plt.subplots(ncols=2, nrows=1, dpi=300)\n",
        "    fig.set_size_inches(9, 3)\n",
        "    ax1, ax2 = axes[0], axes[1]\n",
        "\n",
        "\n",
        "    ax1.plot(train_acc,'-o',label=\"train\", markersize=4)\n",
        "    ax1.plot(test_acc,'--+',label=\"test\", markersize=4)\n",
        "    ax1.plot()\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(loc=3)\n",
        "\n",
        "    ax2.plot(train_loss,'-o',label=\"train\", markersize=4)\n",
        "    ax2.plot(test_loss,'--+',label=\"test\", markersize=4)\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(loc=1)\n",
        "\n",
        "    #plot parameters\n",
        "    #ax1.set_ylim(0, np.max([np.max(train_acc),np.max(test_acc)])+0.1)\n",
        "    #ax2.set_ylim(-0.1, np.max([np.max(train_loss),np.max(test_loss)])+0.1)\n",
        "    ax1.grid(True, which='both',linewidth=0.1)\n",
        "    ax2.grid(True, which='both',linewidth=0.1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c291f5",
      "metadata": {
        "id": "f2c291f5"
      },
      "outputs": [],
      "source": [
        "create_acc_loss_graph2(train_Loss,train_ac,test_Loss,test_ac)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4337ae97",
      "metadata": {
        "id": "4337ae97"
      },
      "source": [
        "### 3.4. Exercise <a class=\"anchor\" id=\"part4\"></a>\n",
        "\n",
        "Now it is time to get your hands dirty and do some coding. Repeat the process above to train the model for noisy data. How the training changes?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68982284",
      "metadata": {
        "id": "68982284"
      },
      "source": [
        "### Important note\n",
        "\n",
        "In case you want to retrain your model for any purposes, you must **restart** you kernell first because you previous training data is already saved on your memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63042f90",
      "metadata": {
        "id": "63042f90"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}